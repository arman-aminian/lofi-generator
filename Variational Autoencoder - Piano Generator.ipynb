{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a866e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIANO_GENERATOR_MODEL_PATH = 'piano_model'\n",
    "PIANO_GENERATOR_MODEL_WEIGHTS_PATH = './piano models/piano_model_variational_autoencoder_weights'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ebf7960",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import glob\n",
    "import pickle\n",
    "import numpy\n",
    "import pandas\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from pydub import AudioSegment\n",
    "from music21 import converter, instrument, note, chord, stream\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Activation\n",
    "from keras.layers import BatchNormalization as BatchNorm\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a300e350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_notes(path):\n",
    "    \"\"\" Get all the notes and chords from the midi files in the 'path' directory \"\"\"\n",
    "    notes = []\n",
    "\n",
    "    print(len(glob.glob(path + '/*.mid')))\n",
    "    for file in glob.glob(path + '/*.mid'):\n",
    "        midi = converter.parse(file)\n",
    "\n",
    "#         print('parsing %s' % file)\n",
    "\n",
    "        notes_to_parse = None\n",
    "\n",
    "        try: # file has instrument parts\n",
    "            s2 = instrument.partitionByInstrument(midi)\n",
    "            notes_to_parse = s2.parts[0].recurse() \n",
    "        except: # file has notes in a flat structure\n",
    "            notes_to_parse = midi.flat.notes\n",
    "\n",
    "        for element in notes_to_parse:\n",
    "            if isinstance(element, note.Note):\n",
    "                notes.append(str(element.pitch))\n",
    "            elif isinstance(element, chord.Chord):\n",
    "                notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "\n",
    "    with open('data/notes', 'wb') as filepath:\n",
    "        pickle.dump(notes, filepath)\n",
    "\n",
    "    return notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3881220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_notes(path):\n",
    "    \"\"\" Get all the notes and chords from the midi files in the 'path' directory \"\"\"\n",
    "    notes = []\n",
    "\n",
    "    print(len(glob.glob(path + '/*.mid')))\n",
    "    for file in glob.glob(path + '/*.mid'):\n",
    "        midi = converter.parse(file)\n",
    "\n",
    "#         print('parsing %s' % file)\n",
    "\n",
    "        notes_to_parse = None\n",
    "\n",
    "        try: # file has instrument parts\n",
    "            s2 = instrument.partitionByInstrument(midi)\n",
    "            notes_to_parse = s2.parts[0].recurse() \n",
    "        except: # file has notes in a flat structure\n",
    "            notes_to_parse = midi.flat.notes\n",
    "\n",
    "        for element in notes_to_parse:\n",
    "            if isinstance(element, note.Note):\n",
    "                notes.append(str(element.pitch))\n",
    "            elif isinstance(element, chord.Chord):\n",
    "                notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "\n",
    "    with open('data/notes', 'wb') as filepath:\n",
    "        pickle.dump(notes, filepath)\n",
    "\n",
    "    return notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "8c9c5760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_idx(idx, axis):\n",
    "    grid = np.ogrid[tuple(map(slice, idx.shape))]\n",
    "    grid.insert(axis, idx)\n",
    "    return tuple(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "141dea9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_initialization(a, ncols):\n",
    "    out = np.zeros(a.shape + (ncols,), dtype=int)\n",
    "    out[all_idx(a, axis=2)] = 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "633c6900",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(notes, n_vocab):\n",
    "    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n",
    "    sequence_length = 32\n",
    "\n",
    "    # get all pitch names\n",
    "    pitchnames = sorted(set(item for item in notes))\n",
    "\n",
    "     # create a dictionary to map pitches to integers\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "\n",
    "    network_input = []\n",
    "    \n",
    "    # create input sequences and the corresponding outputs\n",
    "    for i in range(0, len(notes) - sequence_length, 1):\n",
    "        sequence_in = notes[i:i + sequence_length]\n",
    "        network_input.append([note_to_int[char] for char in sequence_in])\n",
    "\n",
    "    ncols = max(max(network_input))+1\n",
    "    network_input = onehot_initialization(np.array(network_input), ncols)\n",
    "    return network_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "9236c3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93\n"
     ]
    }
   ],
   "source": [
    "notes = get_notes('midi_songs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "9a9b246b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab = len(set(list(chain.from_iterable(notes))))\n",
    "network_input = prepare_sequences(notes, n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "dca32e35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1868, 32, 254)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "7f4cad45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0421e36",
   "metadata": {},
   "source": [
    "## Model - Variational Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a4368e",
   "metadata": {},
   "source": [
    "### Sampling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4d44102e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f52dab4",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7122ad40",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = layers.Conv1D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(network_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "99c334f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1868, 16, 32])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5aadf26b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1868, 512])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = layers.Conv1D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a75bbaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = layers.Dense(16, activation=\"relu\")(x)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z = Sampling()([z_mean, z_log_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "976ea17d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1868, 16])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "63203117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1868, 1]), TensorShape([1868, 1]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_mean.shape, z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ad82f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b84c04b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = layers.Dense(8 * 64, activation=\"relu\")(latent_inputs)\n",
    "x = layers.Reshape((8, 64))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d07fefa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 8, 64])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c124d109",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = layers.Conv1DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Conv1DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "decoder_outputs = layers.Conv1DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ff4b936c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 32, 32])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "222914ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 32, 1])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6200ae7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cdd3f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "05b523cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_26 (InputLayer)           [(None, 32, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 16, 32)       128         input_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 8, 64)        6208        conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 512)          0           conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 16)           8208        flatten_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 2)            34          dense_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 2)            34          dense_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "sampling_10 (Sampling)          (None, 2)            0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 14,612\n",
      "Trainable params: 14,612\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 2\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(32, 1))\n",
    "x = layers.Conv1D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
    "x = layers.Conv1D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(16, activation=\"relu\")(x)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfc9978",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ac6e12d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_27 (InputLayer)        [(None, 2)]               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 512)               1536      \n",
      "_________________________________________________________________\n",
      "reshape_14 (Reshape)         (None, 8, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_transpose_26 (Conv1DT (None, 16, 64)            12352     \n",
      "_________________________________________________________________\n",
      "conv1d_transpose_27 (Conv1DT (None, 32, 32)            6176      \n",
      "_________________________________________________________________\n",
      "conv1d_transpose_28 (Conv1DT (None, 32, 1)             97        \n",
      "=================================================================\n",
      "Total params: 20,161\n",
      "Trainable params: 20,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "x = layers.Dense(8 * 64, activation=\"relu\")(latent_inputs)\n",
    "x = layers.Reshape((8, 64))(x)\n",
    "x = layers.Conv1DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Conv1DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "decoder_outputs = layers.Conv1DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf48220",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "52ea5f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    keras.losses.binary_crossentropy(data, reconstruction)\n",
    "                    , axis=1\n",
    "                )\n",
    "            )\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c27bf5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_mean, z_log_var, z = encoder(network_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "96fa5994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1868, 1]), TensorShape([1868, 1]), TensorShape([1868, 1]))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_mean.shape, z_log_var.shape, z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7d2178de",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction = decoder(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c4cc53a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    keras.losses.binary_crossentropy(network_input, reconstruction)\n",
    "                    , axis=1\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "027459df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1868])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.reduce_sum(\n",
    "                    keras.losses.binary_crossentropy(network_input, reconstruction)\n",
    "                    , axis=1\n",
    "                )\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "20d91f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=39707.547>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstruction_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "00c0ee96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1868, 1])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "kl_loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ee96adf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "kl_loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3bc13fe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.5830584>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5871d5e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7263250b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "dad57197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "edaf7df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.3 ],\n",
       "       [ 2.25],\n",
       "       [ 7.5 ],\n",
       "       [ 2.25],\n",
       "       [ 6.25],\n",
       "       [ 2.25],\n",
       "       [ 4.85],\n",
       "       [11.  ],\n",
       "       [ 4.85],\n",
       "       [ 2.25],\n",
       "       [ 0.7 ],\n",
       "       [ 1.65],\n",
       "       [ 2.25],\n",
       "       [ 6.25],\n",
       "       [ 2.25],\n",
       "       [ 7.2 ],\n",
       "       [11.05],\n",
       "       [ 5.45],\n",
       "       [ 2.45],\n",
       "       [11.75],\n",
       "       [11.5 ],\n",
       "       [11.05],\n",
       "       [10.15],\n",
       "       [ 7.2 ],\n",
       "       [11.05],\n",
       "       [ 0.8 ],\n",
       "       [ 2.45],\n",
       "       [ 6.75],\n",
       "       [ 8.4 ],\n",
       "       [ 4.65],\n",
       "       [ 8.4 ],\n",
       "       [11.25]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_input[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2865342e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "59/59 [==============================] - 1s 2ms/step - loss: -39592.5935 - reconstruction_loss: -336707.8750 - kl_loss: 140087.3906\n",
      "Epoch 2/30\n",
      "59/59 [==============================] - 0s 2ms/step - loss: -8155271.4750 - reconstruction_loss: -17687098.0000 - kl_loss: 6539811.5000\n",
      "Epoch 3/30\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 398549030.8000 - reconstruction_loss: -289227488.0000 - kl_loss: 749905856.0000\n",
      "Epoch 4/30\n",
      "59/59 [==============================] - 0s 2ms/step - loss: -861245828.2667 - reconstruction_loss: -7298083840.0000 - kl_loss: 3560678912.0000\n",
      "Epoch 5/30\n",
      "59/59 [==============================] - 0s 2ms/step - loss: -25673014050.1333 - reconstruction_loss: -38773874688.0000 - kl_loss: 20879587328.0000\n",
      "Epoch 6/30\n",
      "59/59 [==============================] - 0s 2ms/step - loss: -39140297540.2667 - reconstruction_loss: -92101992448.0000 - kl_loss: 63865286656.0000\n",
      "Epoch 7/30\n",
      "59/59 [==============================] - 0s 2ms/step - loss: -54109333400.5333 - reconstruction_loss: -1030445858816.0000 - kl_loss: 1056702791680.0000\n",
      "Epoch 8/30\n",
      "59/59 [==============================] - 0s 2ms/step - loss: -468000730453.3333 - reconstruction_loss: -3257747374080.0000 - kl_loss: 1583440723968.0000\n",
      "Epoch 9/30\n",
      "59/59 [==============================] - 0s 2ms/step - loss: -7161540391731.2002 - reconstruction_loss: -15186763186176.0000 - kl_loss: 8927712378880.0000\n",
      "Epoch 10/30\n",
      "59/59 [==============================] - 0s 2ms/step - loss: -3284984803328.0000 - reconstruction_loss: -13565113139200.0000 - kl_loss: 7523572121600.0000\n",
      "Epoch 11/30\n",
      "59/59 [==============================] - 0s 2ms/step - loss: -7169063771613.8662 - reconstruction_loss: -16924154003456.0000 - kl_loss: 10380869894144.0000\n",
      "Epoch 12/30\n",
      "59/59 [==============================] - 0s 2ms/step - loss: -224306696465.0667 - reconstruction_loss: -231068073984.0000 - kl_loss: 95222176.0000\n",
      "Epoch 13/30\n",
      "59/59 [==============================] - 0s 2ms/step - loss: -257468120814.9333 - reconstruction_loss: -271405187072.0000 - kl_loss: 136930368.0000\n",
      "Epoch 14/30\n",
      "59/59 [==============================] - 0s 2ms/step - loss: -308002720972.8000 - reconstruction_loss: -352738476032.0000 - kl_loss: 226489680.0000\n",
      "Epoch 15/30\n",
      "59/59 [==============================] - 0s 2ms/step - loss: -496623400823.4667 - reconstruction_loss: -840655372288.0000 - kl_loss: 4711384064.0000\n",
      "Epoch 16/30\n",
      "59/59 [==============================] - 0s 2ms/step - loss: -45013675671552.0000 - reconstruction_loss: -73350760103936.0000 - kl_loss: 41354528817152.0000\n",
      "Epoch 17/30\n",
      "59/59 [==============================] - 0s 2ms/step - loss: -61313295974.4000 - reconstruction_loss: -58870767616.0000 - kl_loss: 1310917.0000\n",
      "Epoch 18/30\n",
      "59/59 [==============================] - 0s 2ms/step - loss: -54315986193.0667 - reconstruction_loss: -56017440768.0000 - kl_loss: 1213418.7500\n",
      "Epoch 19/30\n",
      "59/59 [==============================] - 0s 2ms/step - loss: -58878913672.5333 - reconstruction_loss: -59011825664.0000 - kl_loss: 1250329.7500\n",
      "Epoch 20/30\n",
      "59/59 [==============================] - 0s 2ms/step - loss: -61415223705.6000 - reconstruction_loss: -60977229824.0000 - kl_loss: 1287713.1250\n",
      "Epoch 21/30\n",
      "59/59 [==============================] - 0s 2ms/step - loss: -65497958263.4667 - reconstruction_loss: -65420279808.0000 - kl_loss: 1361261.3750\n",
      "Epoch 22/30\n",
      "59/59 [==============================] - 0s 2ms/step - loss: -64724467438.9333 - reconstruction_loss: -65893216256.0000 - kl_loss: 1389338.1250\n",
      "Epoch 23/30\n",
      "59/59 [==============================] - 0s 2ms/step - loss: -68965498197.3333 - reconstruction_loss: -69493186560.0000 - kl_loss: 1438657.7500\n",
      "Epoch 24/30\n",
      "59/59 [==============================] - 0s 2ms/step - loss: -69011578743.4667 - reconstruction_loss: -70463676416.0000 - kl_loss: 1501055.1250\n",
      "Epoch 25/30\n",
      "59/59 [==============================] - 0s 2ms/step - loss: -74712817937.0667 - reconstruction_loss: -75577147392.0000 - kl_loss: 1569040.5000\n",
      "Epoch 26/30\n",
      "59/59 [==============================] - 0s 2ms/step - loss: -76847525478.4000 - reconstruction_loss: -77999890432.0000 - kl_loss: 1639786.5000\n",
      "Epoch 27/30\n",
      "59/59 [==============================] - 0s 2ms/step - loss: -80342818269.8667 - reconstruction_loss: -78577778688.0000 - kl_loss: 1730396.6250\n",
      "Epoch 28/30\n",
      "59/59 [==============================] - 0s 2ms/step - loss: -84136205380.2667 - reconstruction_loss: -82914238464.0000 - kl_loss: 1817397.7500\n",
      "Epoch 29/30\n",
      "59/59 [==============================] - 0s 2ms/step - loss: -87177752029.8667 - reconstruction_loss: -87555268608.0000 - kl_loss: 1935243.6250\n",
      "Epoch 30/30\n",
      "59/59 [==============================] - 0s 2ms/step - loss: -87241720763.7333 - reconstruction_loss: -89662103552.0000 - kl_loss: 2061857.3750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa6a0ba5e20>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.Adam())\n",
    "vae.fit(network_input, epochs=30, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb540027",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
